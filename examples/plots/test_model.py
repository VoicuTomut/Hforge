"""
Script to compare the hamiltonians generated by the model with the loss they correspond.
"""
import torch

from hforge.utils import load_model_and_dataset_from_directory, generate_prediction, get_object_from_module
from prettytable import PrettyTable
from hforge.plots.plot_matrix import reconstruct_matrix

def main():
    # * Path to the folder where the model and other results are saved.
    directory = r"C:\Users\angel\OneDrive - Universitat de Barcelona\1A. MASTER I\TFM\example_results\usetapprox\example_results_usetapprox_2mp_sharing"
    model_filename = "train_best_model.pt"

    # * Load loss function
    loss_fn_name = "mse_cost_function"
    loss_fn = get_object_from_module(loss_fn_name, "hforge.graph_costfunction")

    # Load model
    model, history , train_dataset, validation_dataset, _ = load_model_and_dataset_from_directory(directory, model_filename, weights_only=False, return_datasets=True)

    # Generate predition
    i = 0
    input_graph = train_dataset[i]
    output_graph = generate_prediction(model, input_graph)

    # === Compute parameters to test the model ===
    # Compute loss function
    target_graph = {
        "edge_index": output_graph["edge_index"],
        "edge_description": input_graph.h_hop,
        "node_description": input_graph.h_on_sites
    }
    model.eval()
    with torch.no_grad():
        train_loss_pred, train_component_losses = loss_fn(output_graph, target_graph)

    # Extract hamiltonians
    input_h = reconstruct_matrix(target_graph["h_hop"], target_graph["h_on_sites"], target_graph["edge_index"])
    predicted_h = reconstruct_matrix(output_graph["edge_description"], output_graph["node_description"], output_graph["edge_index"])

    # === Print the comparison ===
    # print(history.keys()) # dict_keys(['train_loss', 'val_loss', 'train_edge_loss', 'train_node_loss', 'val_edge_loss', 'val_node_loss', 'learning_rate'])
    # print(train_component_losses.keys()) # dict_keys(['edge_loss', 'node_loss'])
    print(f"\nAll data extracted from the same model at epoch {len(history.get("train_loss"))+1}:")
    table = PrettyTable(["Type (train dataset)", "Last model save", "New prediction"])
    table.add_row(["train_loss", f"{history.get("train_loss")[-1]:.2f}", f"{train_loss_pred.item():.2f}"])
    table.add_row(["train_loss_hoppings", f"{history.get("train_edge_loss")[-1]:.2f}", f"{train_component_losses["edge_loss"]:.2f}"])
    table.add_row(["train_loss_onsites", f"{history.get("train_node_loss")[-1]:.2f}", f"{train_component_losses["node_loss"]:.2f}"])
    print(table)

    #? Maybe the parameters have been saved wrongly?


if __name__ == "__main__":
    main()